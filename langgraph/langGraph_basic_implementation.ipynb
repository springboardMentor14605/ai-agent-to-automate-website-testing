{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGnjadqPvi58",
        "outputId": "f83a7cdf-cbd4-42be-c83d-00c5f0d4b561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: **LangGraph** is a library developed by the LangChain team designed to build complex, stateful, and multi-actor applications using Large Language Models (LLMs).\n",
            "\n",
            "While standard LangChain is great for creating linear \"chains\" of actions, LangGraph is built for creating **circular logic** (cycles), which is essential for building sophisticated AI agents.\n",
            "\n",
            "---\n",
            "\n",
            "### 1. Why do we need LangGraph?\n",
            "To understand LangGraph, you have to understand the limitation it solves:\n",
            "\n",
            "*   **Standard LangChain (DAGs):** Most chains are \"Directed Acyclic Graphs\" (DAGs). This means the flow moves in one direction: *Input -> Prompt -> LLM -> Output*.\n",
            "*   **The Problem:** Real-world agents often need to loop. They might try a task, fail, reflect on the error, and try again. Standard chains struggle to manage this \"looping\" state and complex decision-making logic.\n",
            "*   **The Solution:** LangGraph allows you to define a graph where nodes can loop back to each other, maintaining a persistent \"State\" throughout the process.\n",
            "\n",
            "### 2. Core Concepts of LangGraph\n",
            "\n",
            "LangGraph operates on three main pillars:\n",
            "\n",
            "#### A. State\n",
            "The **State** is a shared data structure (like a Python dictionary or a Pydantic class) that represents the current \"memory\" of your graph. Every node in the graph reads the state, modifies it, and passes it on.\n",
            "\n",
            "#### B. Nodes\n",
            "**Nodes** are simply Python functions or LangChain \"Runnables.\" Each node performs a specific task, such as:\n",
            "*   Calling an LLM.\n",
            "*   Searching the web.\n",
            "*   Updating a database.\n",
            "*   Formatting a response.\n",
            "\n",
            "#### C. Edges\n",
            "**Edges** define the path between nodes. There are two types:\n",
            "*   **Normal Edges:** \"After Node A finishes, always go to Node B.\"\n",
            "*   **Conditional Edges:** \"After Node A finishes, look at the result. If the LLM says 'I'm done,' go to the End. If the LLM says 'I need more info,' go to the Search Node.\"\n",
            "\n",
            "---\n",
            "\n",
            "### 3. Key Features\n",
            "\n",
            "1.  **Cycles (Loops):** The ability to create iterative processes. For example: *Research -> Critique -> Revise -> Critique again.*\n",
            "2.  **Persistence (Checkpoints):** LangGraph can automatically save the state of the graph after every step. This allows for \"error recovery\" (if the app crashes, it resumes where it left off) and \"human-in-the-loop\" interactions.\n",
            "3.  **Human-in-the-loop:** You can program the graph to **pause** and wait for a human to approve an action (like a tool call or a budget spend) before continuing.\n",
            "4.  **Fine-grained Control:** Unlike the older LangChain \"Agents\" (which were often \"black boxes\"), LangGraph forces you to define the flow explicitly, making it much easier to debug and customize.\n",
            "\n",
            "---\n",
            "\n",
            "### 4. A Simple Workflow Example: A Research Agent\n",
            "\n",
            "Imagine an agent that writes a blog post:\n",
            "\n",
            "1.  **Node 1 (Planner):** Generates an outline.\n",
            "2.  **Node 2 (Researcher):** Searches the web for facts.\n",
            "3.  **Node 3 (Writer):** Writes the draft.\n",
            "4.  **Conditional Edge (Reviewer):** Checks the draft.\n",
            "    *   *If the draft is poor:* Loop back to **Node 2** to get better facts.\n",
            "    *   *If the draft is good:* Go to **End**.\n",
            "\n",
            "---\n",
            "\n",
            "### 5. When should you use it?\n",
            "\n",
            "*   **Use LangChain** if you have a simple, linear pipeline (e.g., a basic RAG application that retrieves a document and answers a question).\n",
            "*   **Use LangGraph** if you are building:\n",
            "    *   **Agents** that need to use tools and reason about the results.\n",
            "    *   **Multi-agent systems** (where one LLM talks to another).\n",
            "    *   **Long-running processes** that require human intervention or state persistence.\n",
            "\n",
            "### Summary\n",
            "LangGraph turns LLM development into **flowchart programming**. It gives you the flexibility of a custom-coded state machine with the power of LangChainâ€™s integrations.\n",
            "Count: 1\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langgraph langchain-google-genai\n",
        "\n",
        "import os\n",
        "# IMPORTANT: Replace \"your-key\" with your actual Google API Key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCafyJTJr6jaabRaLWOqmqn-2Q-0T1pyg4\"\n",
        "\n",
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# class AgentState(TypedDict):\n",
        "#   user_input: str\n",
        "#   agent_response: str\n",
        "\n",
        "# AGENT STATE\n",
        "class AgentState(TypedDict):\n",
        "    user_input: str\n",
        "    agent_response: str\n",
        "    interaction_count: int\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "  model=\"gemini-3-flash-preview\",\n",
        "  temperature=0\n",
        ")\n",
        "\n",
        "# def agent_node(state: AgentState) -> AgentState:\n",
        "#   try:\n",
        "#     response = llm.invoke([\n",
        "#       HumanMessage(content=state[\"user_input\"])\n",
        "#     ])\n",
        "#   except Exception as e:\n",
        "#     print(f\"Error invoking LLM: {e}\")\n",
        "#     return {\n",
        "#       \"user_input\": state[\"user_input\"],\n",
        "#       \"agent_response\": f\"Error: Failed to get response from LLM. Please check your API key and model access. Details: {e}\"\n",
        "#     }\n",
        "\n",
        "#   if isinstance(response.content, list):\n",
        "#     text = \"\".join(\n",
        "#     block[\"text\"] for block in response.content\n",
        "#     if block.get(\"type\") == \"text\"\n",
        "#   )\n",
        "#   else:\n",
        "#     text = response.content\n",
        "\n",
        "#   return {\n",
        "#     \"user_input\": state[\"user_input\"],\n",
        "#     \"agent_response\": text\n",
        "#   }\n",
        "\n",
        "def agent_node(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke([\n",
        "        HumanMessage(content=state[\"user_input\"])\n",
        "    ])\n",
        "\n",
        "    if isinstance(response.content, list):\n",
        "        text = \"\".join(\n",
        "            block[\"text\"] for block in response.content\n",
        "            if block.get(\"type\") == \"text\"\n",
        "        )\n",
        "    else:\n",
        "        text = response.content\n",
        "\n",
        "    return {\n",
        "        \"user_input\": state[\"user_input\"],\n",
        "        \"agent_response\": text,\n",
        "        \"interaction_count\": state[\"interaction_count\"] + 1\n",
        "    }\n",
        "\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"agent\", agent_node)\n",
        "graph.set_entry_point(\"agent\")\n",
        "graph.add_edge(\"agent\", END)\n",
        "app = graph.compile()\n",
        "\n",
        "# result = app.invoke({\n",
        "#   \"user_input\": \"Explain LangGraph in simple terms\",\n",
        "#   \"agent_response\": \"\"\n",
        "# })\n",
        "\n",
        "#HARDCODED USER INPUT\n",
        "# result = app.invoke({\n",
        "#     \"user_input\": \"What is the purpose of LangGraph?\",\n",
        "#     \"agent_response\": \"\"\n",
        "# })\n",
        "\n",
        "#DYNAMIC USER INPUT\n",
        "# user_text = input(\"Enter your question: \")\n",
        "\n",
        "# result = app.invoke({\n",
        "#     \"user_input\": user_text,\n",
        "#     \"agent_response\": \"\"\n",
        "# })\n",
        "\n",
        "#Agent in a Loop\n",
        "# while True:\n",
        "#     user_text = input(\"You: \")\n",
        "\n",
        "#     if user_text.lower() == \"exit\":\n",
        "#         print(\"Exiting agent.\")\n",
        "#         break\n",
        "\n",
        "#     result = app.invoke({\n",
        "#         \"user_input\": user_text,\n",
        "#         \"agent_response\": \"\"\n",
        "#     })\n",
        "\n",
        "#     print(\"Agent:\", result[\"agent_response\"])\n",
        "\n",
        "result = app.invoke({\n",
        "    \"user_input\": \"Explain LangGraph\",\n",
        "    \"agent_response\": \"\",\n",
        "    \"interaction_count\": 0\n",
        "})\n",
        "\n",
        "print(\"Response:\", result[\"agent_response\"])\n",
        "print(\"Count:\", result[\"interaction_count\"])\n",
        "\n",
        "\n",
        "# print(result[\"agent_response\"])"
      ]
    }
  ]
}